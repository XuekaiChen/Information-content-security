# 基于BERT的文本隐写检测

随着自然语言处理技术的飞速发展，文本合成隐写技术得到了很大的发展。这些方法可以分析大量训练样本的统计特征分布，然后生成符合这种统计分布的隐写文本。对于这些隐写方法，以往的隐写分析方法检测性能都不理想。本实验利用深度学习算法搭建快速有效的文本隐写分析方法来解决这个问题。经已有论文研究表明，利用生成的隐写文本中单词之间的相关性是很好的隐写分析判别特征，因此将每个词映射到一个语义空间，并使用隐藏层提取这些词之间的相关性，再根据提取的相关特征进行文本分类，便可以兼备良好的检测精度与实时的检测速度。

本实验使用BERT预训练语言模型将词向量编码，后对句首的'[CLS]'向量接入全连接层进行分类，效果优于RNN、CNN等神经网路模型。
